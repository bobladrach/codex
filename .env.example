# ===========================================================
# Ari’el Codex — Example Environment Variables
# Copy this file to `.env` and adjust values as needed.
# ===========================================================

# --- GPU / CPU Control ---
# Force CPU even if CUDA is detected.
ARIEL_FORCE_GPU=0

# Hide all CUDA devices (equivalent to disabling GPU use).
CUDA_VISIBLE_DEVICES=-1

# --- LLM Integration (Ollama) ---
# Host:port for Ollama if you want to connect Codex to an LLM.
# Default Ollama uses 127.0.0.1:11434. If that port is busy, set a custom one.
# Example:
# OLLAMA_HOST=127.0.0.1:11435
# OLLAMA_MODEL=gemma2:2b

# --- Server Settings ---
# Default bind address and port.
HOST=127.0.0.1
PORT=8000

# --- Verbose / Debugging ---
# Set to 1 to enable verbose logs from startup.
VERBOSE=0

# --- Voice / TTS ---
# Default voice selection (if supported by this build).
VOICE=female

# --- Online / Offline Behavior ---
# Control whether chat tries to use online LLM backend by default.
ONLINE_MODE=True
ONLINE_STICKY=True
